{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3230f9",
   "metadata": {},
   "source": [
    "# Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df2cb0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d72a0d",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bf07e3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "npz = np.load('Audiobooks_data_train.npz')\n",
    "train_inputs = npz['inputs'].astype('float')\n",
    "train_targets =npz['targets'].astype('int')\n",
    "train_data = npz\n",
    "\n",
    "npz = np.load('Audiobooks_data_validation.npz')\n",
    "validation_inputs = npz['inputs'].astype('float')\n",
    "validation_targets =npz['targets'].astype('int')\n",
    "\n",
    "npz = np.load('Audiobooks_data_test.npz')\n",
    "test_inputs = npz['inputs'].astype('float')\n",
    "test_targets =npz['targets'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c4924b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(447, 10)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69f66c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0,\n",
       "       1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1,\n",
       "       1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1,\n",
       "       0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1,\n",
       "       1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a18097f",
   "metadata": {},
   "source": [
    "# Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a037221",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 10\n",
    "output_size = 2\n",
    "hidden_layer_size = 100\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'),\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax')\n",
    "])\n",
    "\n",
    "#optimizer_learning_rate = 0.002\n",
    "#custom_optimizer = tf.keras.optimizers.Adam(learning_rate=optimizer_learning_rate)\n",
    "\n",
    "'''\n",
    "    binary_crossentropy - when binary encoding was applied\n",
    "    categorical_crossentropy - when one-hot encoding was applied\n",
    "    sparse_categorical_crossentropy - automatically applies one-hot encoding\n",
    "'''\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b9fd39",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc1402ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "28/28 - 1s - loss: 0.5309 - accuracy: 0.7234 - val_loss: 0.4456 - val_accuracy: 0.7785 - 933ms/epoch - 33ms/step\n",
      "Epoch 2/100\n",
      "28/28 - 0s - loss: 0.4246 - accuracy: 0.7776 - val_loss: 0.4069 - val_accuracy: 0.7964 - 95ms/epoch - 3ms/step\n",
      "Epoch 3/100\n",
      "28/28 - 0s - loss: 0.3941 - accuracy: 0.7949 - val_loss: 0.3888 - val_accuracy: 0.7875 - 99ms/epoch - 4ms/step\n",
      "Epoch 4/100\n",
      "28/28 - 0s - loss: 0.3794 - accuracy: 0.8039 - val_loss: 0.3773 - val_accuracy: 0.7987 - 99ms/epoch - 4ms/step\n",
      "Epoch 5/100\n",
      "28/28 - 0s - loss: 0.3706 - accuracy: 0.8055 - val_loss: 0.3683 - val_accuracy: 0.8076 - 95ms/epoch - 3ms/step\n",
      "Epoch 6/100\n",
      "28/28 - 0s - loss: 0.3656 - accuracy: 0.8061 - val_loss: 0.3690 - val_accuracy: 0.8098 - 98ms/epoch - 4ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c3eea5df60>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 128\n",
    "MAX_EPOCHS = 100\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=1\n",
    ")\n",
    " \n",
    "model.fit(\n",
    "    train_inputs, \n",
    "    train_targets, \n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=MAX_EPOCHS,\n",
    "    callbacks=[early_stopping],\n",
    "    validation_data=(validation_inputs,validation_targets),\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681a1fb8",
   "metadata": {},
   "source": [
    "# Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ff8a2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14/14 [==============================] - 0s 2ms/step - loss: 0.3808 - accuracy: 0.7902\n",
      "Test loss: 0.38 - Test accuracy: 79.02%\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_occuracy = model.evaluate(test_inputs, test_targets)\n",
    "print('Test loss: {0:.2f} - Test accuracy: {1:.2f}%'.format(test_loss,test_occuracy*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3-TF2.0] *",
   "language": "python",
   "name": "conda-env-py3-TF2.0-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
